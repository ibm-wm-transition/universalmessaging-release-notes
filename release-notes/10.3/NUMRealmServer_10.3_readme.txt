                     Universal Messaging webMethods RealmServer 10.3 Fix 24 Readme
                 
                                           January 2022

____________________________________________________________________________________________________

1.0   Fix Name
2.0   Fix ID
3.0   Product(s)/Component(s) Affected
4.0   Requirements and Dependencies
5.0   Platform Support
6.0   Cautions and Warnings 
7.0   Fix Contents
8.0   Installation
9.0   Uninstallation
10.0  Globalization 
11.0  Copyright
____________________________________________________________________________________________________


1.0 Fix Name

Universal Messaging webMethods RealmServer 10.3 Fix 24



2.0 Fix ID

NUM_10.3_wM_RealmServer_Fix_24



3.0 Product(s)/Component(s) Affected

Universal Messaging webMethods RealmServer 10.3 



4.0 Requirements and Dependencies

None.



5.0 Platform Support

Same platforms as product release.



6.0 Cautions and Warnings

It is strongly advisable that the Universal Messaging client libraries are patched together with the 
realm server binaries, and that both are kept up to date with latest Universal Messaging official 
fix set.


7.0 Fix Contents

In addition to including resolutions to product defects, a fix may also include enhancements to 
existing functionality and features. Any of these items can result in new or changed built-in 
services, APIs, or configuration parameters. Review the fix contents carefully before installing the 
fix.

This fix is cumulative, which means that it includes all previous fixes for the release.

NUM-17288 (NUM_10.3_wM_RealmServer_Fix_24)

A very rare condition in slave recovery could leave a slave node with channel(s) not recovered 
resulting in OOM. This issue is now resolved. 

NUM-17150 (NUM_10.3_wM_RealmServer_Fix_24)

After migrating to Universal Messaging version 10.1 or above, a secondary node goes down because the 
connection used for a multiplex session stalls.
The issue is resolved.

NUM-17062 (NUM_10.3_wM_RealmServer_Fix_24)

During the cluster destruction process, there could be a communication task executing, that updates 
a server's known state of remote nodes. There was a possibility that those two processes could 
interfere with each other, resulting in a NullPointerException logged in the server log file. This 
issue is now resolved.

NUM-16158 (NUM_10.3_wM_RealmServer_Fix_24)

Universal Messaging does not provide an option to export Protobuf
definitions from a channel or queue.
The Protobuf definitions are needed when you want to explore
the structure of a payload in a client application or when you
import or export the contents of a channel or queue.
The issue is resolved. Now you can export Protobuf definitions 
from a channel or queue in the following ways:
- Using a new command-line tool named ExportProtobufDefinitions.
- Using the Enterprise Manager. A new option, Export Protobuf Definitions,
has been added to the context menu for a channel or queue.
In addition, the ExportRealmXML and ImportRealmXML command-line tools
also have options for exporting and importing Protobuf definitions, respectively.

NUM-14326 (NUM_10.3_wM_RealmServer_Fix_24)

After an exclusive durable gets deleted, the client might still receive events from the deleted 
durable.
The issue is resolved.

NUM-11809 (NUM_10.3_wM_RealmServer_Fix_24)

When clients use the Universal Messaging channel iterator client API to consume events from
a channel with a shared durable, pausing a session to the server might cause messages to be 
perceived as lost.
The issue occurs because the server does not suspend subscriptions properly when a session is 
paused.
In this case, the server might send events to consumers while the consumers are not ready to process 
them.
Such events are perceived as lost, but will be redelivered once the session is closed and a new one 
is established.
This issue is resolved. Now the Universal Messaging server times out all durable subscriptions
when a session is paused, and clients receive a nRequestTimedOutException.

NUM-10065 (NUM_10.3_wM_RealmServer_Fix_24)

When non-durable JMS sync consumer on channel was re-subscribed and there are no new events 
published- it was possible the UM server to not delete the old events kept by the old subscriber.
This is now fixed- when the new consumer is added it will properly reset its interest and the old 
events for which we don't have interest anymore will be deleted.

NUM-16830 (NUM_10.3_wM_RealmServer_Fix_23)

The Universal Messaging realm server has memory and performance issues when a memory leak occurs in 
the task scheduler.
The issue is resolved. 

NUM-16785 (NUM_10.3_wM_RealmServer_Fix_23)

Having a username that contains "@" results in Universal Messaging logging the exception message 
"Unexpected subject/address format.?, which might flood the nirvana.log file.
The issue is now resolved.

NUM-16772 (NUM_10.3_wM_RealmServer_Fix_23)

When a Prometheus agent tries to get the attribute values for a new realm interface on a Universal 
Messaging server with JMX enabled, the server logs a NullPointerException into the nirvana.log 
file.
The issue is resolved. Now if the new realm interface is not started, the JMX Adapter bean for the 
Universal Messaging JMX Server returns an empty string.

NUM-16744 (NUM_10.3_wM_RealmServer_Fix_23)

The Universal Messaging realm server has memory and performance issues when a memory leak occurs in 
the task scheduler.
The issue is resolved. 

NUM-16658 (NUM_10.3_wM_RealmServer_Fix_23)

Previously, log entries "Acknowledging an event we have no record" would be printed in UM server 
logs suggesting an issue with consumer registration, when in fact, these log entries would appear 
in all cases when indexed durable consumption takes place. The log message has been changed to be 
more descriptive and its log level has been changed to trace. 

NUM-16549 (NUM_10.3_wM_RealmServer_Fix_23)

A slave node might enter a recovery loop and fail to join a Universal Messaging cluster.
The issue might occur when the cluster has a channel of type Mixed and events bigger than 10 MB 
exist on the channel.
The issue is now resolved.

NUM-16523 (NUM_10.3_wM_RealmServer_Fix_23)

When a durable is corrupted, Universal Messaging shuts down with an error message that does not 
include the name of the corrupted .idx or .txn file.
The server returns the following general error in the nirvana.log file: "Server shutdown initiated 
due to fatal file
I/O error occurred : Failed during openOrCreate null."
The issue is resolved. Server logging has been improved to include the name of the corrupted durable 
file.

NUM-16517 (NUM_10.3_wM_RealmServer_Fix_23)

Creating and then deleting many channels or queues might result in Universal Messaging leaking 
memory and shutting down. 
The issue is now resolved. 

NUM-16497 (NUM_10.3_wM_RealmServer_Fix_23)

Recovering empty indexed durables in a cluster might result in failed inter-realm communication 
between the master node and a secondary node.
The issue can occur when no events are waiting to be delivered on the durable or the number of 
events on the master and the secondary node is equal.
The issue is resolved.

NUM-16457 (NUM_10.3_wM_RealmServer_Fix_23)

Despite using a network drive as a data directory, Universal Messaging might store some .mem files 
locally. 
Universal Messaging might incorrectly save some .mem files on a local drive, even though you are 
using a network drive as a data directory for storing channels and queues. 
The issue is now resolved.

NUM-16367 (NUM_10.3_wM_RealmServer_Fix_23)

Recovering empty indexed durables in a cluster might result in failed inter-realm communication 
between the master node and a secondary node.
The issue can occur when no events are waiting to be delivered on the durable or the number of 
events on the master and the secondary node is equal.
The issue is resolved.

NUM-16350 (NUM_10.3_wM_RealmServer_Fix_23)

Server status log now will display the open file descriptor on Unix platform

NUM-16160 (NUM_10.3_wM_RealmServer_Fix_23)

Issue with high disk usage has been resolved.

NUM-15668 (NUM_10.3_wM_RealmServer_Fix_23)

Universal Messaging reports a ConcurrentModificationException due to synchronization issues with 
multi-file disk stores.
The issue is resolved.

NUM-12566 (NUM_10.3_wM_RealmServer_Fix_23)

When a shared or serial durable subscriber is added to a channel, with a selector that is already 
registered with the channel, the operation fails with the exception 
'com.pcbsys.nirvana.client.nIllegalArgumentException: No change detected in selector'
The issue is now resolved.

NUM-10472 (NUM_10.3_wM_RealmServer_Fix_23)

Creating a new session to Universal Messaging might fail when working with temporary stores that 
have the autoDelete flag enabled.
Session creation fails with the message "com.pcbsys.nirvana.client.nUnexpectedResponseException:
Received an unexpected response to the request: Unexpected response to handshake request,
class com.pcbsys.nirvana.base.events.nDeleteChannel".
The issue is resolved.

NUM-16488 (NUM_10.3_wM_RealmServer_Fix_22)

A thread pool scheduler regression introduced in 10.3 Fix 21 could cause the queue timeout manager 
task to get stuck and block consumer threads, for example, Integration Server. This issue is now 
resolved. 

NUM-16232 (NUM_10.3_wM_RealmServer_Fix_22)

A very rare deadlock in the server could lead to the cluster falling apart when queues or shared 
queued durables are used. This issue is now fixed. 
This defect cannot occur in 10.5 and newer releases where the old style queues are removed. 

NUM-16145 (NUM_10.3_wM_RealmServer_Fix_22)

An issue with high CPU and memory usage is observed in Universal Messaging.
The issue is resolved.

NUM-16097 (NUM_10.3_wM_RealmServer_Fix_22)

EM was showing internal session ID related information as connection key in the durable tab for 
indexed durables. This prevented end users from finding potentially slow or stuck consumers to 
bounce. The fix is to display IP:Port SessionID formatted key instead of internal session ID info 
so that client connections could be identified in EM and bounced. 

NUM-16028 (NUM_10.3_wM_RealmServer_Fix_22)

Caching for multi-file disk stores gets disabled after applying NUM_10.3.0_wM_RealmServer_Fix17.
The disabled cache affects the persistent event throughput and latency performance on a highly 
loaded Universal Messaging server with a multi-file disk store.
The issue is resolved.

NUM-15970 (NUM_10.3_wM_RealmServer_Fix_22)

When nProtobuf events get published from a Universal Messaging server to the client, the events are 
not processed in the correct order. The issue occurs because the custom Protobuf attributes are not 
set in the nProtobuf events.
The issue is resolved.

NUM-15963 (NUM_10.3_wM_RealmServer_Fix_22)

The performance of a Universal Messaging cluster might deteriorate when a shared-queued durable 
consumer sends event acknowledgements to a clustered realm.
The problem occurs because the master realm broadcasts event acknowledgements to all nodes in the 
cluster and then waits for the realms to process the acknowledgements.
This issue is now resolved. The Universal Messaging master realm no longer waits for the clustered 
realms to respond to an acknowledgement request.

NUM-15961 (NUM_10.3_wM_RealmServer_Fix_22)

Universal Messaging returns a NullPointerException at startup.
Universal Messaging writes a NullPointerException in the nirvana.log at startup if the server tries 
to create a cluster-wide internal store while the cluster is not configured.
The issue is resolved.

NUM-15798 (NUM_10.3_wM_RealmServer_Fix_22)

Out of memory could occur in rare cases because the Finalizer thread could be locked out by the UM 
channel purge mechanism. This issue is now resolved. 

NUM-15729 (NUM_10.3_wM_RealmServer_Fix_22)

In selectors for protobuf events, the values in integer arrays are not compared properly.
This issue is now resolved.

NUM-15716 (NUM_10.3_wM_RealmServer_Fix_22)

When events get published from a Universal Messaging server to a channel, the durable subscription 
does not receive the events from a shared queue with a missing join to the channel.
The shared queue for the durable could miss a join to a channel when the master server in the 
cluster goes down during a join recovery. In such cases, the new (elected) master in the cluster 
might have inconsistent information about the joins.
The issue is resolved.

NUM-15431 (NUM_10.3_wM_RealmServer_Fix_22)

The Universal Messaging realm server has performance issues when insufficient threads are assigned 
to the task scheduler.
The issue is resolved. Now the default value of "SchedulerPoolSize" is 10. After installing the fix, 
the scheduler pool size gets updated to the default value (if it was less than 10) or remains 
unchanged (if it was greater than 10).
Doc versions have also been updated with the new value.

NUM-15404 (NUM_10.3_wM_RealmServer_Fix_22)

Universal Messaging might shut down with an out-of-memory error.
In some cases, the Universal Messaging realm server might shut down with OutOfMemoryError. The issue 
occurs due to a combination of factors including the slow processing of data buffers from 
multiplexed sessions.
The issue is resolved.

NUM-15393 (NUM_10.3_wM_RealmServer_Fix_22)

The Universal Messaging server shuts down with the following error message: "Multi file store, reset 
file headers due to underlying exception, key:409000."
The issue is resolved.
The multi-file store now fails when the store order is inconsistent and the error message includes 
the store path.
Also, the full stack trace with the error details is added in the wrapper log.

NUM-15360 (NUM_10.3_wM_RealmServer_Fix_22)

Sparse purging of events from a channel might cause high CPU usage in Universal Messaging
when the channel has a lot of shared durables.
Sparse purging of events from a channel might become very time-consuming when the channel has
a lot of shared durables and some of the durables are not active. This can lead to the
accumulation of many events on the durables and high CPU usage.
The issue is resolved.

NUM-15348 (NUM_10.3_wM_RealmServer_Fix_22)

UM generates several thread dumps due to Reason: Stalled task detected on this thread pool.
Fixed the code logic to calculate the task duration correctly, this issue has been resolved.

NUM-15170 (NUM_10.3_wM_RealmServer_Fix_22)

A large number of indexed durables on a Universal Messaging channel might cause high CPU usage and 
performance issues.
When a channel has multiple indexed durables, but only several of them are active, events might 
accumulate causing memory and performance issues.
The issue is resolved.

NUM-15163 (NUM_10.3_wM_RealmServer_Fix_22)

Universal Messaging writes messages to already rolled nirvana.log files.
The problem occurs because Universal Messaging does not use the correct logging framework. The 
server uses the default logging framework instead of the additionally specified Log4J2 framework.
The issue is resolved. Now all packages use the correct logger for the package level.

NUM-13600 (NUM_10.3_wM_RealmServer_Fix_22)

Fixed an issue with false-positive slow consumer warning for long-living subscriptions to Shared and 
Serial durables.

NUM-13204 (NUM_10.3_wM_RealmServer_Fix_22)

Session logout messages in nirvana.log does not show the milliseconds information for client session 
duration.
This issue is addressed. The  session log out messages now show the millisecond information.

NUM-10694 (NUM_10.3_wM_RealmServer_Fix_22)

In a Universal Messaging cluster, when a secondary realm with a cluster-wide serial or shared 
durable subscription disconnects from the cluster, the master realm might fail to properly clean up 
the resources allocated for this subscription on behalf of the disconnected realm.
This issue is now resolved.

NUM-8058 (NUM_10.3_wM_RealmServer_Fix_22)

When disk scanning is enabled, Universal Messaging shuts down and does not start if the free disk 
space threshold is reached even when the remaining disk space is sufficient.
The issue is resolved. Now when you enable disk scanning, Universal Messaging behaves in the 
following way:
- The server safely shuts down with an error message when the free disk space is less than 500 MB.
- The server logs a warning message when the free disk space is less than the disk usage free 
threshold percentage.

NUM-15858 (NUM_10.3_wM_RealmServer_Fix_21)

Very large server data folders with tens of thousands shared queued durables were causing cluster 
formation to time out, fail and retry continously. This was due to multiple root causes and they 
are now fixed. 

NUM-14748 (NUM_10.3_wM_RealmServer_Fix_21)

When executing server shutdown script now the JMX server will be shutdown as well.

NUM-15721 (NUM_10.3_wM_RealmServer_Fix_20)

Possible dead lock fixed during initializing Master cluster state.
It was possible during master state initialization the JMS engine to dispatches a purge cluster-wide 
purge request, because of the dead lock the master state initialization to never finish. The 
deadlock was introduced with the "Disconnect On Cluster Failure" server-side flag feature.

NUM-15542 (NUM_10.3_wM_RealmServer_Fix_20)

When client is performing nTransaction#commit inside a cluster on a slave node and at the same time 
the channel is deleted on the master node the master will not respond to the slave which will lead
to the connection between these two being interrupted after the MasterRequestTimeout has passed. 
This can lead to downtime in the UM cluster. 
This issue is now fixed.

NUM-15534 (NUM_10.3_wM_RealmServer_Fix_20)

When the web socket frame buffer is empty and the server received closed operation, the server will 
try to get the size of frame buffer and will return NPE.
This issue is now resolved
Also, the warning log message "Porotocl: HTTPD: Invalid protocol handshake detected. Has cookie but 
no such session exists" is now improved to return the cookie the server received.

NUM-15466 (NUM_10.3_wM_RealmServer_Fix_20)

A very large number of shared queued durables each one with a filter could lead to a very slow 
server startup. This issue is now fixed. 

NUM-15426 (NUM_10.3_wM_RealmServer_Fix_20)

UM server logs could be flooded with Slave NOT OK to purge messages. This issue is now resolved. 

NUM-15324 (NUM_10.3_wM_RealmServer_Fix_20)

Due to a large number of latent consumers with large window sizes and large events, there could be a 
pile up of events on NirvanaClusterQueueCommitChannel (NCQCC). 
This could gradually load a cluster and client commit request timeouts could begin to occur. This 
issue is now resolved. 

NUM-15322 (NUM_10.3_wM_RealmServer_Fix_20)

Fixed a tiny miscalculation in the synchronous queue consumers client-side event caching mechanism.

NUM-15294 (NUM_10.3_wM_RealmServer_Fix_20)

Client connection leaks could occur on server potentially leading to out of memory when client 
connections try to reconnect at a very high rate. This issue is now resolved. 

NUM-15260 (NUM_10.3_wM_RealmServer_Fix_20)

Universal Messaging clustered realm may encounter a deadlock if a client is publishing to a queue in 
a transactional fashion and other client(s) which have subscribed to the queue are meanwhile 
getting disconnected from the server.
This issue is now resolved.

NUM-15224 (NUM_10.3_wM_RealmServer_Fix_20)

A Universal Messaging cluster node might fail to exit recovery state.
A cluster node might remain in recovery if the difference between the
first and last event ID on a channel on the node is too large.
The issue is resolved.

NUM-15197 (NUM_10.3_wM_RealmServer_Fix_20)

The Universal Messaging RealmInformationCollector tool fails to run in a Docker container.
When you try to start the RealmInformationCollector in a Docker container, the tool returns
the following error: 'Universal Messaging realm server component not present in the current
installation directory (/opt/softwareag/UniversalMessaging/tools/runner/./../../..). 
So this operation is not applicable.' 
This issue is now resolved.

NUM-15071 (NUM_10.3_wM_RealmServer_Fix_20)

Fixed an issue which was causing Admin sessions to be disconnected if connected to Slave nodes 
during enabling the Replication Cluster mode. The sessions were able to reconnect, so it was an 
intermediate disconnect. This is now resolved and Admin sessions are not affected on Replication 
cluster mode enabling

NUM-14968 (NUM_10.3_wM_RealmServer_Fix_20)

Fixed a potential Queue cluster mismatch caused by multiple subscribe\unsubscribes done to one and 
same queue from one client session . If the slave node detects that there was already a queue 
consumer it was preventing creation of the consumer to avoid memory leaks. This was wrong as the 
master node already created the subscription, thus the slave need to just accept master's decision. 
Actions were taken and to ensure the memory leak check is still done properly.
Presence of the following warning log message in the slave's server logs would be an indication that 
this particular issue occurred. After the fix this log message can still be present, but will not 
be an indication of a problem.

NUM-14876 (NUM_10.3_wM_RealmServer_Fix_20)

Address message loss scenarios for remote cluster joins when:
a. A cluster wide channel is deleted and recreated. 
b. A remote cluster join connects to a slave instead of the master node (the solution here is to 
force all remote cluster join connections to connect to the master node of a destination cluster).

NUM-14814 (NUM_10.3_wM_RealmServer_Fix_20)

A defect in server logic could lead to "Clearing old request class 
com.pcbsys.nirvana.server.events.nClusterQueueEvent" log entries. This issue is now fixed. 

NUM-14782 (NUM_10.3_wM_RealmServer_Fix_20)

Fixed the issue with ncb file handling which can cause the unnamed durables to be added in case of 
issue with file system.

NUM-14767 (NUM_10.3_wM_RealmServer_Fix_20)

Exclusive durables are not always properly recovered during cluster recovery. 
This issue is now resolved.

NUM-14132 (NUM_10.3_wM_RealmServer_Fix_20)

Stacktraces for "Treating the request as asynchronous" could be logged in server logs when a node 
transitions from SLAVE to OFFLINE and then to MASTER state. This is a very rare defect with minor 
impact. It is now fixed. 

NUM-12267 (NUM_10.3_wM_RealmServer_Fix_20)

Added the instance check before casting the event to nPublished type

NUM-11779 (NUM_10.3_wM_RealmServer_Fix_20)

When the Cluster Config -> DisconnectWhenNotReady is set to true 
and the UM server has not joined the cluster- the UM server was also disconnecting and Admin 
connections. Admin connections should not be disconnected in this case and this is now resolved.
Now If the Admin client tries to perform cluster-wide operation while the UM server has not joined 
the cluster yet- the client will get a exception on the cluster-wide operation, instead of closing 
the session.  

NUM-14690 (NUM_10.3_wM_RealmServer_Fix_18)

An Universal Messaging realm joining a cluster may not recover index durable information from 
master, causing inconsistent outstanding and/or pending events on the durable. This could cause 
inability to consume events from the durable on the impacted slave.
The issue can surfice when Universal Messaging master realm does not have any oustanding and pending 
events on a durable and a slave realm is re-joining the cluster and undergoing recovery.
This issue is now resolved. Universal Messaging master realm will now unconditionally send durable 
information to realms under recovery, even if there are no outstanding and pending events on the 
durable.

NUM-14535 (NUM_10.3_wM_RealmServer_Fix_18)

When there are non cluster wide durables on a master node inside UM cluster when any of the slaves 
gets into recovery it may not finish and "Cluster> Error: Index recovery can not find index durable 
with ID:" log messages can be seen in the slave's log file. This issue is now fixed.

NUM-14412 (NUM_10.3_wM_RealmServer_Fix_18)

There was an issue with the connection establishment between cluster nodes, when setting cluster 
communications ACL entries in interface VIA list. This issue is now resolved. 

NUM-14311 (NUM_10.3_wM_RealmServer_Fix_18)

Security vulnerabilities were detected in Universal Messaging:
- A hardcoded password and username that are never
actually used. The issue occurs when you use plugins for Universal
Messaging.
- Vulnerabilities when you use JAAS authentication.
The issues are resolved.
- The hardcoded password and username have been removed.
- JAAS authentication is now disabled by default. You can use a new system
property,"Nirvana.enable.legacy.jaas.auth", to enable JAAS authentication. 
Property description
Nirvana.enable.legacy.jaas.auth
Enables or disables JAAS authentication.
Values are:
- true - enables JAAS authentication.
- false (default) - disables JAAS authentication.
You set the new property in the Server_Common.conf file in the
<SoftwareAG_directory>\UniversalMessaging\server\<serverInstanceName>\bin
directory in the following format: 
wrapper.java.additional.<n>=-DNirvana.enable.legacy.jaas.auth=true | false
where <n> is a unique positive integer.
If you do not set this property, JAAS authentication is disabled by default.

NUM-14094 (NUM_10.3_wM_RealmServer_Fix_18)

Google protocol buffers Java library is updated to version 3.7.1

NUM-13735 (NUM_10.3_wM_RealmServer_Fix_18)

In some corner cases, the last event acked is not updated properly in the UM resulting purge not 
happening a expected and there by doc built up.
This issue is now resolved.

NUM-13474 (NUM_10.3_wM_RealmServer_Fix_18)

Universal Messaging realm server with disabled authentication may reject client connections over 
nhps with client certificate due to a conflict of the suplied user principal and the client 
certificate common name.
This issue is now resolved and the behavior when connecting over nhps is the same as when connecting 
over nsps - the server will use the client certificate CN as the user principal.

NUM-14427 (NUM_10.3_wM_RealmServer_Fix_17)

Create a Disconnect On Cluster Failure server-side flag which will default to false, but if set will 
override the client one: nSessionAttributes.setFollowTheMaster()
The flag which will control the Disconnect On Cluster Failure server-side behaviour is this one: 
Cluster Config->DisconnectWhenNotReady.
When set to True the UM server which is goes to OfflineState will trigger a disconnect of all 
Non-Admin client connections. 
This server connections disconnect will prevent further processing of user events, which makes UM's 
cluster behaviour more deterministic.
When the flag is False the default behaviour is that the decision for particular user events 
processing is made withing the particular event handlers- every handler might decide to handle the 
OfflineState situation differently, so behaviour is not always clear.
Setting the Cluster Config->DisconnectWhenNotReady to true need to be considered if customer's A/A 
Cluster setup doesn't involve using a non-cluster-wide resources within the cluster.

NUM-14323 (NUM_10.3_wM_RealmServer_Fix_17)

The creation of an nRealmNode instance might fail with nSessionNotConnectedException. 
When you use the Universal Messaging Admin API to connect to a Universal Messaging realm server you 
create an nRealmNode instance. If the realm server is part of a cluster that is still in the process 
of forming, the nRealmNode instance successfully connects to the server but then might disconnect 
from it. This results in an nSessionNotConnectedException error, even if the server is available.
The issue is resolved.

NUM-14218 (NUM_10.3_wM_RealmServer_Fix_17)

Enhance logging while closing the HTTP driver due to inactivity.

NUM-14187 (NUM_10.3_wM_RealmServer_Fix_17)

Enhance logging while closing the HTTP driver due to inactivity.

NUM-14170 (NUM_10.3_wM_RealmServer_Fix_17)

A warning log entry was added to warn if there are many non-matching events in a queue for a 
subscriber with filter .
This case often cause hidden performance issues at customer side, so we are making it visible from 
the logs
We log the messages(in Warning log level) when there are more then 10 000 non-matching events at the 
beginning of the store.
We also log if the pop succeeded with at least one matching event.
These log entries will help customers to find wrongly configured queue subscribers with filters and 
help with the RCA which was relatively hard.

NUM-14169 (NUM_10.3_wM_RealmServer_Fix_17)

Optimised is the Queue purge speed. Until now purging events from a queue was requiring the entire 
event to be loaded before being purged.
This had a big impact when purging events from huge queues. This is now optimised and event is not 
read before being purged.
The issue is not applicable for channels or queues in v10.5 and newer.

NUM-14084 (NUM_10.3_wM_RealmServer_Fix_17)

A rare cluster recovery issue could lead to the queue mismatch log file being filed up with queue 
inconsistency log entries reducing performance and cluster throughput significantly. This issue is 
now fixed. 

NUM-13861 (NUM_10.3_wM_RealmServer_Fix_17)

When a User Purge was initiated from a Slave node using client API or Enterprise manager it was 
possible some events on the slave node to be left if during the purge there is active 
Non-transactional publish on the Master node.
This is now fixed

NUM-12911 (NUM_10.3_wM_RealmServer_Fix_17)

It was possible to get inconsistent store state on a cluster-wide channel\queue if TTL or 
Capacity(Honour Capacity=false) are set. This was because the expired events were cleared 
individually on each cluster nodes. Not the purging of the expired events is done cluster-wide, 
i.e. only the Master node has the authority to initiate purge of these expired events.

NUM-14191 (NUM_10.3_wM_RealmServer_Fix_16)

Thread dumps generated by the UM server are missing information regarding object monitors and 
ownable synchronizers currently locked by the thread.
The issue is now resolved.

NUM-14189 (NUM_10.3_wM_RealmServer_Fix_16)

When using Universal Messaging server with custom security groups, CPU and memory usage gradually 
increases over time.
This issue is now resolved.

NUM-14156 (NUM_10.3_wM_RealmServer_Fix_16)

Universal Messaging Enterprise Manager shuts down unexpectedly when connecting
to an NHPS or NSPS interface.
Universal Messaging Enterprise Manager shuts down unexpectedly when connecting
to an NHPS or NSPS interface with disabled client certificate validation. The
issue happens when you specify valid values for the CAKEYSTORE and CAKEYSTOREPASSWD
environment variables in the Admin_Tools_Common.conf file and leave the CKEYSTORE
system property in the nenterprisemgr.conf file empty.
The issue is resolved. 

NUM-11708 (NUM_10.3_wM_RealmServer_Fix_16)

Make stalled task timeout and the thread dump configurable.
Thread pool monitoring is used to detect stalled task at fixed time interval of 60 second. This 
timeout has been made configurable to allow user to specify the timeout for stalled and slow task. 
Also user can now configure the thread dump interval.

NUM-14091 (NUM_10.3_wM_RealmServer_Fix_15)

Fixed issue, where when converting a store with publish keys(in this case JNDI) from local to 
clusterwide can result in failure and cluster recovery can't finish properly because of it.

NUM-14073 (NUM_10.3_wM_RealmServer_Fix_15)

NPE can happen while server is processing addition of a connection listener to a channel with 
indexed durable and in the meantime a new async subscriber is connected to this durable. This issue 
is now fixed

NUM-13810 (NUM_10.3_wM_RealmServer_Fix_15)

When the user updates MaxDirectMemorySize from the Command Central UI, the UM server does not show 
the updated value after server restart.
This issue is fixed now. 

NUM-13745 (NUM_10.3_wM_RealmServer_Fix_15)

An asynchronous non-transactional queue consumer unsubscribing from a cluster-wide queue may cause 
consumer state to leak on slave realms and only be cleared when the underlying session is closed.
This issue is now resolved.

NUM-13687 (NUM_10.3_wM_RealmServer_Fix_15)

UM Provider filters with multiple ESCAPE character ( LIKE '% A/_A/_A %' ESCAPE '/', LIKE 
'A\\__\\_A\\_A' ESCAPE '\\' etc.,) was not working. 
With this fix, UM Provider filters with multiple ESCAPE character will be working.

NUM-12473 (NUM_10.3_wM_RealmServer_Fix_15)

Improved API documentation for the transient flag of nConsumeEvent.

NUM-13804 (NUM_10.3_wM_RealmServer_Fix_14)

We have tested the zero cluster downtime support using three UM nodes in a cluster and certification 
passed successfully.
We highly recommend customers to start applying the new fixes on the Slave nodes and leave the 
Master node to be their last one.

NUM-13731 (NUM_10.3_wM_RealmServer_Fix_14)

When starting instance without setting data directory property we are searching in  config.props. If 
we can't find it here we were logging the exception. Now we changed to log it when property in 
config.props is found, because we don't expect it to be found often.
This issue is resolved.

NUM-13640 (NUM_10.3_wM_RealmServer_Fix_14)

Session.getChannels does not return valid channel attributes
Fixed the issue for autodelete and jms engine attributes.

NUM-13540 (NUM_10.3_wM_RealmServer_Fix_14)

A clustered Universal Messaging server might return an IndexOutOfBoundsException.
A Universal Messaging server that is part of a cluster might write an 
IndexOutOfBoundsException to the server log. The issue occurs when the server is
processing a request from another server in the cluster and in the meantime 
the second server disconnects from the cluster.
The issue is resolved. 

NUM-13534 (NUM_10.3_wM_RealmServer_Fix_14)

Universal Messaging might become unresponsive to clients.
Universal Messaging might become unresponsive and stop publishing events to 
clients. The issue occurs when a client connects to Universal Messaging while the server is 
retrieving
the current client connections to which events should be published.
The issue is resolved. 

NUM-13418 (NUM_10.3_wM_RealmServer_Fix_14)

Universal Messaging server may not send status updates to admin connections which are established 
during server startup.
Universal Messaging server initializes system data groups when running status update thread for the 
first time and any admin connections which are established before the thread is started, will not 
receive server status updates.
The issue is now resolved, Universal Messaging server will initialize system data groups during 
server startup rather than when status monitor thread is run.

NUM-13128 (NUM_10.3_wM_RealmServer_Fix_14)

When a client connection to Universal Messaging server is closed, stack trace of the exception is 
printed in nirvana.log. This fills up the log file.
This issue is addressed. The stack trace is not printed when the connection is closed. 

NUM-13447 (NUM_10.3_wM_RealmServer_Fix_13)

Command Central displays UNRESPONSIVE status for clustered Universal Messaging servers after
you apply the same migration template twice.
After you apply a template to migrate a Universal Messaging cluster in Command Central more than 
once with the environment.mode parameter
set to MIGRATION and the migration.type parameter set to CROSS_HOST, the second time the Universal 
Messaging servers have UNRESPONSIVE status in Command Central.
The issue is resolved.

NUM-13241 (NUM_10.3_wM_RealmServer_Fix_13)

The handling of subscribing with an invalid filter was incorrect and resulted in request timeout, 
without any other clues as to what went wrong. This is now fixed by responding the the client 
accordingly.

NUM-13087 (NUM_10.3_wM_RealmServer_Fix_13)

When using framework "LOG4J2" or "LOGBACK" log file is not created if not explicity set via 
-DLOGFILE. This issue is now fixed.

NUM-11228 (NUM_10.3_wM_RealmServer_Fix_13)

A cluster requires strong consistency of all resources, during a node coming into slave state it 
recovers all resources and brings them into line with the master to achieve this consistency. When 
temporary channels are recovered there was a small chance of a ConcurrentModification being logged 
and the recovery not being completed correctly. This issue is now resolved.

NUM-10899 (NUM_10.3_wM_RealmServer_Fix_13)

Improvements in the way missing channel errors are handled when subscribing to a channel/queue in a 
cluster.

NUM-13361 (NUM_10.3_wM_RealmServer_Fix_12)

Two new JVM system properties are added to enable or disable SASL reverse resolve for localhost. 
Both on server and client side:
- Nirvana.sasl.client.localhostResolve
- Nirvana.sasl.server.localhostResolve
The default value for both is True, i.e. they are enabled. Setting those two properties to false 
makes sense when there are SASL related client connection exceptions over NSP. 

NUM-13258 (NUM_10.3_wM_RealmServer_Fix_12)

When new logger was created, the passed fileName parameter was always ignored if the sys prop 
LOGFILE was set. This was preventing creating different logs with different names.
This in particular was an issue with the QueueMismatchLogger, available in versions older than 10.5 
when StoreDiagnostics is enabled.

NUM-13254 (NUM_10.3_wM_RealmServer_Fix_12)

Universal Messaging shows ? gradual decrease in free memory to warning levels and,
on occasion, high CPU usage.?
When the Universal Messaging server has many connections that are?
processing intensive event flows, the heap or direct memory, depending on the?
UseDirectBuffering connection property, gradually decreases to warning?
levels and CPU usage might increase. The issue occurs especially when the value of the BufferSize 
connection?
property is higher than 10240 bytes.?
The issue is resolved. 

NUM-13146 (NUM_10.3_wM_RealmServer_Fix_12)

Two new JVM system properties are added to enable or disable SASL reverse resolve for localhost. 
Both on server and client side:
- Nirvana.sasl.client.localhostResolve
- Nirvana.sasl.server.localhostResolve
The default value for both is True, i.e. they are enabled. Setting those two properties to false 
makes sense when there are SASL related client connection exceptions over NSP. 

NUM-13114 (NUM_10.3_wM_RealmServer_Fix_12)

When a slave is disconnected from master there are ghost connections which are not closed. As a 
consequence some events are not rolled back to the new connections and events are never being 
consumed.
The issue is solved.

NUM-13023 (NUM_10.3_wM_RealmServer_Fix_12)

Fixed issue where when using JS client with driver "XHR_LONGPOLL_CORS",
the server does not set appropriate headers on response.

NUM-12852 (NUM_10.3_wM_RealmServer_Fix_12)

It was possible that rollbacked event from a Queue on which Capacity is set(HonourCapacity=false) 
can be lost if the queue was already full. This is resolved- now the rollback will pass.

NUM-12767 (NUM_10.3_wM_RealmServer_Fix_12)

Universal Messaging cluster formation fails because the cluster enters a state of
endless recovery. 
The issue occurs because a queue on a slave node in the cluster keeps
receiving the same events from the master and cannot recover.
The issue is resolved. 

NUM-11830 (NUM_10.3_wM_RealmServer_Fix_12)

When there are no consumers for the channel, published events to channel the will be moved to Dead 
event store. 

NUM-12924 (NUM_10.3_wM_RealmServer_Fix_11)

Universal Messaging does not update the license expiration date properly.
If you add a new invalid license for Universal Messaging before the old one has
expired, the license manager keeps the expiration date of the old license, and
the server does not shut down within 30 minutes due to an invalid or missing
license.
The issue is resolved. 

NUM-12922 (NUM_10.3_wM_RealmServer_Fix_11)

In case of queue synchronization problems and event ID inconsistency between master and slave, event 
pile up on slaves could occur. This issue is now fixed. 

NUM-12921 (NUM_10.3_wM_RealmServer_Fix_11)

Closing a consumer connection for a cluster-wide queue may leave stale subscription state on master 
realm. If a queue consumer is connected to a slave realm and disconnects, the master realm may not 
properly clean up the subscription state for this consumer.
This issue is now resolved.

NUM-12863 (NUM_10.3_wM_RealmServer_Fix_11)

Fixed an issue where a selector using JMS properties (JMS message ID etc..)
does not work after republishing an event.

NUM-12651 (NUM_10.3_wM_RealmServer_Fix_11)

NUM-12651
When restarting a cluster node, it is possible for a remote cluster join to never connect properly 
to a destination cluster, thus leaving events stuck on the source cluster.
This issue is now resolved.

NUM-12510 (NUM_10.3_wM_RealmServer_Fix_11)

Streaming of log entries from server to Enterprise Manager or other Admin API client could lead to 
OOM. This issue is now fixed. 

NUM-12448 (NUM_10.3_wM_RealmServer_Fix_11)

UM running high memory.
The issue was due to lot number if inflight documents causing delay in memory clean and in some 
cases caching of events was also done wrongly.
This issue is resolved, now event caching is not done in case store caching is disabled.

NUM-12869 (NUM_10.3_wM_RealmServer_Fix_10)

Performance degradation is observed when publishing to a channel with one or more shared-queued 
durables.

NUM-12864 (NUM_10.3_wM_RealmServer_Fix_10)

Fixed an issue, where when using transactional publish to slave we could hit a race condition server 
side, which may cause stream corruption.

NUM-12812 (NUM_10.3_wM_RealmServer_Fix_10)

UM running high memory.
The issue was due to lot number if inflight documents causing delay in memory clean and in some 
cases caching of events was also done wrongly.
This issue is resolved, now event caching is not done in case store caching is disabled.

NUM-12801 (NUM_10.3_wM_RealmServer_Fix_10)

Universal Messaging server will fail to process an event if it has an invalid Google protocol buffer 
dictionary set and event has to be filtered.
This issue is now resolved. If Universal Messaging server fails to decode a Google protocol buffer 
event, for example due to event payload not matching the protocol definition, the event will still 
be processed by the server, meaning that the server will be able to store  the event. However if 
Google protocol buffer event filtering is enabled, any subscribers which specify a selector might 
not receive this event.

NUM-12791 (NUM_10.3_wM_RealmServer_Fix_10)

When removing a shared durable or serial durable subscriber from a channel, Universal Messaging may 
throw an exception "com.pcbsys.nirvana.client.nUserNotSubscribedException: User is not subscribed 
to the channel:RemoveSubscribe" which is returned as a response to the client application even 
though the subscriber is disconnected correctly.

NUM-12728 (NUM_10.3_wM_RealmServer_Fix_10)

For remote joins, the source node may change, but the destination side may still hold a connection 
to the previous node/realm.  This has the unfortunate side effect of leaving queued events on the 
source channel, since the destination side is now acknowledging events on a former node. We need to 
reset this behavior by always referring to the incoming connection, since this new connection will 
always be up to date (i.e. always point to the source node).

NUM-12693 (NUM_10.3_wM_RealmServer_Fix_10)

Channel ACL values in the Enterprise Manager and ACL values in the exported
document representing a realm do not match.
When you create a channel in the Enterprise Manager, the default user is granted
all ACL permissions for the channel. However, when you export the realm to 
an XML file, the value representing the Named permission is set to "false". 
The issue is resolved.

NUM-12618 (NUM_10.3_wM_RealmServer_Fix_10)

Some log entries were present in nirvana.log file with FATAL log level while they are kind of 
informative so most of these are now moved into LOG level. 

NUM-12597 (NUM_10.3_wM_RealmServer_Fix_10)

When removing a shared durable or serial durable subscriber from a channel, Universal Messaging may 
throw an exception "com.pcbsys.nirvana.client.nUserNotSubscribedException: User is not subscribed 
to the channel:RemoveSubscribe" which is returned as a response to the client application even 
though the subscriber is disconnected correctly.

NUM-12550 (NUM_10.3_wM_RealmServer_Fix_10)

When multiple Shared Durable queued subscribers are using one nSession object(on JMS level it is one 
Connection object) then it was possible to leak subscriber objects when the connection gets 
disconnected. This results in daemon subscribers left on the server and in case these daemon 
consumers had pending events in them these events would be never released untill the server is 
restarted

NUM-12448 (NUM_10.3_wM_RealmServer_Fix_10)

UM running high memory.
The issue was due to lot number if inflight documents causing delay in memory clean and in some 
cases caching of events was also done wrongly.
This issue is resolved, now event caching is not done in case store caching is disabled.

NUM-12561 (NUM_10.3_wM_RealmServer_Fix_9)

Clustered channel joins are missing after you restart Universal Messaging.
When you have joined channels in two clusters and restart all realms in both
clusters, the source channel joins are missing after the restart.
The issue is resolved. 

NUM-12494 (NUM_10.3_wM_RealmServer_Fix_9)

JMS Engine now has the ability to purge individual events from channels which has only Index based 
consumers. Currently these consumers are the Serial and the new Shared ones.
The individual JMS Engine purge is not enabled by default. To enable it new configuration is added: 
"Event Storage"->JMSEngineIndividualPurgeEnabled = true.
When enabled the JMS engine will check following:
- if only index consumers are used, then all the index durables of the channel will be scanned and 
all common events which are already processed will be purged.
- if there are no Index consumers, then individual purge can not be done and JMS purge engine will 
delete up-to the oldest event ID for which the channel has interest
- if there is a mix of Indexed and non-indexed consumers then individual purge will be performed 
only for the events for which indexed consumers don't have interest and its EID is lower then the 
lowest EID of the non-indexed consumers.
both interest holders- ones which can maintain individual interest and the ones which can't. Based 
on that information the JMS Engine decides if it can do individual purge or not.

NUM-12466 (NUM_10.3_wM_RealmServer_Fix_9)

Fixed a rare case where the recovery process failed to all stores are recovered. There were a path 
where it was possible recovery to complete without being restarted for the stores where it failed.
To identify that this was the case in the UM server's log any of the follwoing log entries will be 
missing after the Slave's recovery: "Setting recovered state for " or "Cluster store recovered from 
master : " for the particular store in question.

NUM-12464 (NUM_10.3_wM_RealmServer_Fix_9)

Migration of already existing durables is not working properly because of new types being introduced 
between the versions. 
Issue is now solved

NUM-12419 (NUM_10.3_wM_RealmServer_Fix_9)

License key of UM server used to expire 24 hours earlier - at the beginning of the expiration date 
instead of the end of the day. This issue is now fixed.

NUM-12295 (NUM_10.3_wM_RealmServer_Fix_9)

NUM-12295
If the source or destination side of a remote join is deleted, then it's possible that, if the same 
join is reused/recreated between the same source and destination, event flow might not occur.
This fix addresses this issue.

NUM-12189 (NUM_10.3_wM_RealmServer_Fix_9)

Added a slow consumers monitoring on the indexed durables(Shared or Serial) which will report if 
there are slow subscribers which are not acking or consuming events in a timely manner.
The log entries are logged in log level Warning(3) and will contain follwoing string: "Consumer 
Warning: Durable analysis message for ".

NUM-12054 (NUM_10.3_wM_RealmServer_Fix_9)

Fixed several Enterprise Manager/Admin API display and update issues with regards to remote joins

NUM-11321 (NUM_10.3_wM_RealmServer_Fix_9)

Retrieving the IP address of IPv6 enabled host was problematic.
This issue is now resolved.

NUM-11136 (NUM_10.3_wM_RealmServer_Fix_9)

NUM-11136
While connecting to a remote server, it was possible to lose the first event that was sent across a 
remote join.
This issue is now resolved.

NUM-11076 (NUM_10.3_wM_RealmServer_Fix_9)

NUM-11076
There exists a possibility of potential message loss if clusters are used as members of a zone, and 
the master node is shutdown.
This issue is now resolved.

NUM-12415 (NUM_10.3_wM_RealmServer_Fix_8)

Earlier only direct memory used by Events was getting reported in the logs and it was not complete 
information as network buffer mostly allocated on direct memory and consumes significant part of 
the direct memory. 
This has been fixed now. 
Direct Memory Monitor and its proper usage now will be reflected in status logs.

NUM-12343 (NUM_10.3_wM_RealmServer_Fix_8)

"The migration procedure from version 9.x to version 10.x allows queues with the JMS engine enabled 
to be migrated. However, from version 10.1 onwards, support for queues with the JMS engine enabled 
has been removed, so the unrestricted migration of such queues should not be allowed.
To solve this problem, Universal Messaging version 10.x ensures that if a migrated queue is 
configured to use the JMS engine, this configuration is automatically changed during server restart 
so that the queue uses the default engine instead.
This issue is fixed now."

NUM-12300 (NUM_10.3_wM_RealmServer_Fix_8)

Serial consumers would not consume correctly messages in batches even when there is a large backlog 
of events. For example, if the batch size is 50, a serial consumer may receive only 2 or 3 messages 
in a single batch instead of all 50. This issue is now resolved. 

NUM-12285 (NUM_10.3_wM_RealmServer_Fix_8)

When parsing a selector, which starts with the String "%_", a StackOverflowError would be thrown. 
This was an unhandled case in the code, which is now corrected and this scenario is working 
properly.

NUM-12274 (NUM_10.3_wM_RealmServer_Fix_8)

Thread dumps generated by the UM server are missing complete call stacks and monitor information.
The issue is now resolved.

NUM-12138 (NUM_10.3_wM_RealmServer_Fix_8)

Rare issue in store recovery could result in ArrayIndexOutOfBoundsException. This problem is now 
fixed. 

NUM-12121 (NUM_10.3_wM_RealmServer_Fix_8)

The plugin logger used to get closed when logging the first encountered exception. This issue is now 
fixed, and the logger works as expected.

NUM-12048 (NUM_10.3_wM_RealmServer_Fix_8)

A synchronous queue reader which receives events in batches (using peekBatchSize > 1), is not able 
to consume when events are published to the queue via a join.
This issue is now resolved.

NUM-11424 (NUM_10.3_wM_RealmServer_Fix_8)

NUM-11424
It was possible for nConnection.handleData to receive an nServerCommsEvent prior to an event handler 
being already established. This could result in a "lost" event if:
a. No event handler is present or available at the time the nServerCommsEvent is read from the 
nPhysicalConnection. 
b. The event is then subsequently queued up in nConnection.handleData. 
c. If no further events are received for processing, then the event that was queued up in item b 
will never get processed, and would appear to be "lost" when viewed from a protocol perspective. 
This was most readily apparent while dealing with zones (i.e. the nRemoteZoneMonitor).
This issue is now resolved. Event handlers are now bound at compile time (i.e. early binding) to 
prevent this "lost" event, versus dynamically or via late binding, which resulted in the 
predicament that was described earlier.

NUM-12087 (NUM_10.3_wM_RealmServer_Fix_7)

Channel mismatch in the last channel event ID could occur immediately after recovery. This issue is 
now fixed. 

NUM-12039 (NUM_10.3_wM_RealmServer_Fix_7)

Channel last event ID mismatch could occur in a cluster when using registered events. This issue is 
now fixed. 

NUM-12038 (NUM_10.3_wM_RealmServer_Fix_7)

Channel last event ID mismatch could occur when using registered events with update flag false. This 
issue is now fixed. 

NUM-12037 (NUM_10.3_wM_RealmServer_Fix_7)

Queue mismatch could occur when disconnecting asynchronous consumers. This issue is now resolved. 

NUM-11965 (NUM_10.3_wM_RealmServer_Fix_7)

Logic for purging events from multi file store is now optimized. Previously, event was read twice 
from disk while this is not really needed. This is now fixed.

NUM-11889 (NUM_10.3_wM_RealmServer_Fix_7)

Upon restart the Universal Messaging server fails to load google protocol buffers definitions larger 
than 1 MB. When such a definition is attached to a channel, the channel reload routine is 
interrupted at the point when it gets to the large definition object.
This issue is resolved.

NUM-11645 (NUM_10.3_wM_RealmServer_Fix_7)

Universal Messaging JMS implementation may leak sessions and consumers when the JMS connection is 
closed via another thread. 
When using asynchronous JMS queue consumers, this could result in a window of events being allocated 
to the JMS session, but not delivered to consumers untill the JMS connection is restarted.
This issue is now resolved.

NUM-11574 (NUM_10.3_wM_RealmServer_Fix_7)

Asynchronous non-transactional queue consumers connected to Universal Messaging master realm might 
cause inconsistent content of "NirvanaClusterQueueCommitChannel" internal store across cluster.
As a result, the size of the "NirvanaClusterQueueCommitChannel" store on slave realms might be 
greater than the size of the same store on the master realm.
This issue is now resolved.

NUM-11963 (NUM_10.3_wM_RealmServer_Fix_6)

Addiitonal logging added in fProcessNIOQueue can flood the logs with stack traces for IOException 
which is expected in certain cases. This logging is now lowered and stack is not being logged for 
such exceptions anymore.

NUM-11945 (NUM_10.3_wM_RealmServer_Fix_6)

Added logging of protobuf exceptions when protobuf message can't be parsed by the protobuf library. 
This makes it easier to investigate a "non-working filters" issues, if they are caused by badly 
compiled protobuf message.

NUM-11895 (NUM_10.3_wM_RealmServer_Fix_6)

Universal Messaging server prints a warning when attempting to send an event to a synchronous 
consumer for a cluster-wide queue, when the consumer uses message selector that does not match the 
event to be send.
The warning message might be one of:
- Thread <name> tried to release channel lock on /RealmSpecific/NirvanaClusterQueueCommitChannel but 
it never acquired the lock.
- Trying to release the channel lock on /RealmSpecific/NirvanaClusterQueueCommitChannel when current 
thread <name> did not allocate
The issue is now resolved. Universal Messaging server will not try to release lock over 
NirvanaClusterQueueCommitChannel if the lock was never acquired.

NUM-11846 (NUM_10.3_wM_RealmServer_Fix_6)

Fixed issue with outstanding event count for channels/queues that use multi-file store.

NUM-11641 (NUM_10.3_wM_RealmServer_Fix_6)

Universal Messaging now supports AMQP 1.0 in clustered environments. Note however that automatic 
failover of AMQP clients is not supported. If the AMQP connection fails it is the responsibility of 
the AMQP client application to reconnect to another node in the cluster.

NUM-11603 (NUM_10.3_wM_RealmServer_Fix_6)

In a scenario where there are multiple durable subscribes to the same channel, there is slow network 
connection and large events there is serialization sending the events to the subscribers which may 
lead to piliing up of write task, holding the channel lock for a long time thus preventing other 
system level events from being processed etc. This is fixed by removing an artificial wait in the 
NIO output stream wrapper.

NUM-11167 (NUM_10.3_wM_RealmServer_Fix_6)

Index Durables inter cluster communication is revisited now to ensure Index management events are 
processed in order and durable objects are in sync after node is recovered. 
 As a result it was possible that a badly recovered slave keep interest of events which are out of 
sync with the events on the Master node.
NOTE: This change introduces a protocol change in the inter-realm communication, thus when upgrading 
from a fix which doesn't contain this fix- a rolling cluster update is not possible. All cluster 
nodes should be stopped, upgraded and re-started.

NUM-10870 (NUM_10.3_wM_RealmServer_Fix_6)

Dead-lock around cluster members update is fixed.

NUM-11924 (NUM_10.3_wM_RealmServer_Fix_5)

A NullPointerException in the the internal data structure holding cached events  in memory can lead 
to a spinning cache maintenance task, which in turn blocks channel activity as it holds the channel 
lock.
The issue is now resolved.

NUM-11739 (NUM_10.3_wM_RealmServer_Fix_5)

Revert cluster throttling fixes introduced by NUM-11709.
It can introduce slow processing of events on the slave, sent from the master, in case of very 
loaded cluster.

NUM-11706 (NUM_10.3_wM_RealmServer_Fix_5)

Fix for NUM-11238 led to correct calculation of one of the flags which determined if the messages 
needed to be stored within the cluster realms. However it exposed another problem in the new 
implemenentation of durables. The issue in new durable implementation is now fixed.

NUM-11702 (NUM_10.3_wM_RealmServer_Fix_5)

UM server failed to roll log file on AIX due to missing classes. The issue is now resolved.

NUM-11701 (NUM_10.3_wM_RealmServer_Fix_5)

Due to a regression, a queue mismatch could occur between a master and slave. This could lead to the 
following known issues - accumulation of messages on slaves, message loss or duplicates when 
consumers are connected to slaves and consuming from cluster wide queues. Those known issues are 
now resolved. 

NUM-11678 (NUM_10.3_wM_RealmServer_Fix_5)

Added GetServerTime command to runUMTool's Miscellaneous category

NUM-11656 (NUM_10.3_wM_RealmServer_Fix_5)

Rolling back on Shared-Queued durables may result in a thread lockout in Universal Messaging server, 
when in parallel another transactional operation (for example publish) is performed on the channel 
where the durable is defined.
This may cause the transactional operation to stall and respective client to fail with a request 
timeout exception.
This issue is now resolved.

NUM-11474 (NUM_10.3_wM_RealmServer_Fix_5)

When connecting or reconnecting native or JMS queue consumers that could create resource leakage 
which could get worse with each queue commit or rollback. With time, this could lead to stuck IS 
triggers or queue consumers not receiving messages any longer. This issue is now resolved. 

NUM-11319 (NUM_10.3_wM_RealmServer_Fix_5)

Fixed an issue where event sent to a queue, could reach the DLQ no matter it was delivered to 
consumers.

NUM-11303 (NUM_10.3_wM_RealmServer_Fix_5)

JAVA_EXEC and WRAPPER_LIB configurations had absolute path which has to be replaced while creating 
docker image since source and destination installation will not always be in the same path. 
This is now resolved 
JAVA_EXEC and  WRAPPER_LIB configuration entries are updated with relative paths. 

NUM-11294 (NUM_10.3_wM_RealmServer_Fix_5)

Eliminated concurrent access in fEventInputStream, which sometimes had the chance to produce 
NullPointerException.

NUM-11240 (NUM_10.3_wM_RealmServer_Fix_5)

After a restart, the Universal Messaging server might fail to recreate
durable subscriptions, which results in lost events.
When a channel has a durable subscription and you restart the realm server,
Universal Messaging might fail to recreate the durable, which results in the loss of events.
The issue is resolved.

NUM-11215 (NUM_10.3_wM_RealmServer_Fix_5)

Due to a regression, a queue mismatch could occur between a master and slave. This could lead to the 
following known issues - accumulation of messages on slaves, message loss or duplicates when 
consumers are connected to slaves and consuming from cluster wide queues. Those known issues are 
now resolved. 

NUM-11155 (NUM_10.3_wM_RealmServer_Fix_5)

Problem in realm UID generation could lead to erratic behavior in cluster processing which might 
very rarely manifest itself in a class cast exception. This issue is now fixed.  

NUM-10283 (NUM_10.3_wM_RealmServer_Fix_5)

A cluster of realms exposing only nsps or nhps interfaces cannot be created when client 
authentication is enabled.
If you try to create a cluster of realms that have only nsps or nhps interfaces and client 
authentication for the interfaces is enabled, the cluster is never formed.
The issue is resolved.

NUM-9964 (NUM_10.3_wM_RealmServer_Fix_5)

Previous implementation of channel deletion via nAdminAPI was resulting in stores being marked as 
external, i.e. federated store, in a particular edge case. Issue is now resolved.

NUM-11551 (NUM_10.3_wM_RealmServer_Fix_4)

Added additional store diagnostics which will log detailed information 
about queue-related operations (for example: popping events from a queue) 
The additional diagnostics are enabled for all stores if
Universal Messaging log level is set to TRACE.
Alternatively, they can be enabled for specific stores by configuring a 
'um.diagnostics.enabled.stores' Java system property.
The value of this property should be a comma-separated list of store names, each name must be a 
fully-qualified queue name, or a queue name prefix which ends with a forward-slash. 
If a store name ends with a forward slash, diagnostics will be enabled for all queues which names 
start with the given prefix. 
You might set the property to extend the list of stores, adding more queues for which diagnostics 
will be logged. 
For example: 
-Dum.diagnostics.enabled.stores=/org/example/,/org/testqueue
The above will turn on the additional diagnostics for all stores 
which start with "/org/example/" prefix and for the "/org/testqueue" one.
Please note that if the store diagnostics are enabled for one or more stores, Universal Messaging 
server performance might drop due to the additional logging involved.

NUM-11455 (NUM_10.3_wM_RealmServer_Fix_4)

Cannot create Consumer when using Message Selector with Durable Subscriber.
There was an issue with handling of an older version of durable subscription with the subscriber.
This issue has been resolved.

NUM-11427 (NUM_10.3_wM_RealmServer_Fix_4)

An OOME could occur when an update event processing thread encountered an exception and stopped 
processing the updates. This resulted in in-memory event pile up and subsequently an OOME. 
This issue is now resolved.

NUM-11394 (NUM_10.3_wM_RealmServer_Fix_4)

Subscription filters were failing when an nProtobuf event was published to a channel in the event 
that the durable subscriber was from an older major UM release on a 10.3 server i.e. 10.1 durable 
subscriber on 10.3 server. Issue is now resolved.

NUM-11270 (NUM_10.3_wM_RealmServer_Fix_4)

Improved the ?rror handling when creating Shared Memory sessions on unsupported platforms.

NUM-11264 (NUM_10.3_wM_RealmServer_Fix_4)

Issue was found to be in the occasional lack of setting of remote interest when such exists 
alongside with local join interest. This is now fixed.

NUM-11242 (NUM_10.3_wM_RealmServer_Fix_4)

HTTP X-Forwarded-For header is ignored when a client connects over nhps protocol. The issue is now 
resolved.

NUM-11239 (NUM_10.3_wM_RealmServer_Fix_4)

In certain situations UM Server might temporarily enter in lockout during sending events. UM clients 
sending events to server might get blocked as well.
This is fixed by some unnecessary contended locks from UM event sending paths

NUM-11220 (NUM_10.3_wM_RealmServer_Fix_4)

Fixed possible NPE problem during shared-queued durable initialization.

NUM-11181 (NUM_10.3_wM_RealmServer_Fix_4)

Problem in synchronization of queue state between a master and slave could lead to events being 
accumulated on slaves event after a purge. The issue is now fixed. 

NUM-11007 (NUM_10.3_wM_RealmServer_Fix_4)

Updated ExitOnMemoryError and ExitOnDiskIOError JVM configuration entries type error.
Now all the JVM entries will be updated on XML import and update through Admin API.

NUM-10236 (NUM_10.3_wM_RealmServer_Fix_4)

Not able to enable JMS connection alias with Create temporary Queue option enabled when connecting 
to UM cluster.
Fixed the code logic for the older version non admin client.
This issue has been resolved.

NUM-11333 (NUM_10.3_wM_RealmServer_Fix_3)

Users were required to be granted the global "Configure" permission when reading the server 
configuration.
The issue is now resolved. The global "Configure" permission is only required when modifying the 
server configuration.

NUM-11166 (NUM_10.3_wM_RealmServer_Fix_3)

Fixed is event throttling in the inter-cluster communication where one node can send events faster 
then other node can process them. This was might lead OOME. Now if event queue become big, the 
receiving node will throttle.

NUM-11161 (NUM_10.3_wM_RealmServer_Fix_3)

Universal Messaging server may lose events for durable subscriptions where multiple JMS consumers 
are frequently rolling back events and/or closing their session. This was possible due to improper 
identification of the last acknowledged event ID and subsequent purging of events which are not yet 
delivered to any consumer.
This issue is now resolved.

NUM-11149 (NUM_10.3_wM_RealmServer_Fix_3)

In case of bidirectional join purge engine was not deleting event from the destination store even if 
there was no interest for it anymore.
There was a similar issue for the join with filter. Event was not getting deleted from the source 
until a matching event was processed by destination.

NUM-11130 (NUM_10.3_wM_RealmServer_Fix_3)

Corruption of RemoteInterest.nst could lead to dynamic joins being unable to recover even after 
multiple server restarts. This issue is now resolved by rebuilding the RemoteInterest.nst file if 
necessary. 

NUM-11113 (NUM_10.3_wM_RealmServer_Fix_3)

Fixed an issue where, when setting up VIA lists on interfaces (through EM or in some cases the admin 
API) connections establishment would be possible as if there were no VIA lists specified.

NUM-10987 (NUM_10.3_wM_RealmServer_Fix_3)

Universal Messaging server might attempt to recover a channel for a connection
which is no longer active, causing the respective recovery task to spin forever. 
The issue is now resolved. The recovery task will be cancelled if the respective connection is no 
longer active.

NUM-10944 (NUM_10.3_wM_RealmServer_Fix_3)

Event purge performance was improved in the case where there is indexed durable(Shared or Serial) 
attached to a store.

NUM-10917 (NUM_10.3_wM_RealmServer_Fix_3)

Slave lattices in server logs could display a stale master after recovery is complete. The issue 
does not lead to cluster instability or any other adverse behavior apart from reporting the cluster 
status wrongly. The issue is now resolved. 

NUM-10624 (NUM_10.3_wM_RealmServer_Fix_3)

Fixed performance issue with the new style shared durables. It was causing slow acknowledgments and 
because of that slower subscription rate as a whole. 

NUM-10123 (NUM_10.3_wM_RealmServer_Fix_3)

Migration 9.6 to 9.12: duplicate execution of process steps - processes still thread safe?
This issue is happening when there is very large documents published
to shared durable and multiple subscriber like integration servers
are processing these documents, In some specific cases auto maintenance
for common QueueCommitChannel which hold the document under processing
caused the delay in publish and subscriber path. Sometime this delay 
cause the transaction publish from Integration Server failed due
to timeout.
Integration Server republish document on timeout causing the issues.
This issue has been resolved by allowing to configure the server to use 
multi file store for server store thus avoiding the auto maintenance.
Following configuration need to be added to server_common.conf to configure
multi file store for server stores. 
wrapper.java.additional.XXXX="-DServerStoresFileSpindleSize=100"
wrapper.java.additional.XXXX.stripquotes=TRUE
where XXXX is the last index + 1 in the server_common.conf.
Note: These configuration need fresh instance or instance clean up by following steps.
- Delete the following storage files from the data directory.
- Channel.nst
- Nirvana*.mem files

NUM-10093 (NUM_10.3_wM_RealmServer_Fix_3)

Channel iterators for shared named objects and serial/shared durables no longer auto acknowledge.

NUM-11058 (NUM_10.3_wM_RealmServer_Fix_2)

Information stored in Environment Config.cfg was failing to be written to disk when the information 
was changed, is now rectified thus correct information should be displayed via enterprise manager 
when a realm is restarted. This includes the case of server migration.

NUM-11048 (NUM_10.3_wM_RealmServer_Fix_2)

A rare corner case in multi file store was causing a realm to shut down due to IO error. This defect 
has never reached production and was fixed in the integration cycle. 

NUM-11031 (NUM_10.3_wM_RealmServer_Fix_2)

A lockout could occur while shutting down a realm. This issue is now resolved. 

NUM-11018 (NUM_10.3_wM_RealmServer_Fix_2)

Reduced log severity of the following log messages:
Durable>Manager> Releasing file from memory?
Durable>Manager> Mapping file to memory ?

NUM-10948 (NUM_10.3_wM_RealmServer_Fix_2)

Migration between local and cluster wide channels using multi file store was not working. Channel 
migration, in general, was working only for non-empty channels. These issues are now fixed. 

NUM-10929 (NUM_10.3_wM_RealmServer_Fix_2)

Support for JMSType header filtering for AMQP events with complete AMQP-JMS transformation.

NUM-10923 (NUM_10.3_wM_RealmServer_Fix_2)

Out of disk space due to a growing MEM file could very rarely occur during recovery when using the 
multi file store. The defect is now fixed.  

NUM-10897 (NUM_10.3_wM_RealmServer_Fix_2)

UM Available Memory slowing decreasing over time for the 2 slaves in the UM cluster.
Issue happen when non persistent event with large TTL value published  and subscribed using 
MIXED_TYPE event store.
This issue has been resolved
Issue is resolved.

NUM-10850 (NUM_10.3_wM_RealmServer_Fix_2)

Fixed a case where queue asynchronous subscribers count is not correct.
The issue is now resolved.

NUM-10813 (NUM_10.3_wM_RealmServer_Fix_2)

Channel iterate or pop queue events could be lost on the server in rare cases leading to stuck 
client threads. This defect is now resolved. 

NUM-10800 (NUM_10.3_wM_RealmServer_Fix_2)

Fixed an issue where when you stop a JMS connection events are being redelivered.
Fixed an issue where when you remove and add asynchronous queue 
subscribers and ack events concurrently results in message loss.
The issue is now resolved.

NUM-10773 (NUM_10.3_wM_RealmServer_Fix_2)

AMQP TTL header field is now mapped to the JMS TTL header field as part of the AMQP complete JMS 
transformation.

NUM-10635 (NUM_10.3_wM_RealmServer_Fix_2)

When configuration property located in any of the config files is different (min/max value / 
description text / warning text) from the same property in the current UM version used this 
property is updated and the default value is assigned. This is now fixed and when the property is 
updated the value is assigned to the one from the config file if possible. If it is out of range 
the corresponding max/min value is assigned.

NUM-10628 (NUM_10.3_wM_RealmServer_Fix_2)

Subscriber name filtering is disabled whenever a provider filter is updated in Integration Server.
The issue is now resolved.

NUM-10545 (NUM_10.3_wM_RealmServer_Fix_2)

Acknowledge and rollback operations on shared and serial durable objects in a cluster are now 
handled asynchronously on slave nodes in order to avoid blocking the inter-realm communication.

NUM-10236 (NUM_10.3_wM_RealmServer_Fix_2)

Not able to enable JMS connection alias with Create temporary Queue option enabled when connecting 
to UM cluster.
Fixed the code logic for the older version non admin client.
This issue has been resolved.

NUM-9010 (NUM_10.3_wM_RealmServer_Fix_2)

Fixed a rear Queue mismatch of a shared durable queue. During node recovery it was possible to 
end-up with one event mismatch between the newly recovered slave and the other two nodes. If there 
are "Unable to find event" log entries in server logs for NamedSubscriber queues- this could be a 
possible reason.

NUM-8125 (NUM_10.3_wM_RealmServer_Fix_2)

The subscription to an exclusive durable is now handled robustly when the durable is cluster-wide.

NUM-10696 (NUM_10.3_wM_RealmServer_Fix_1)

Fixing an issue where a node could get stuck in Recovery. 

NUM-10569 (NUM_10.3_wM_RealmServer_Fix_1)

Cluster recovery could get stuck due to out of sync inter-realm communication.
This issue is now resolved.

NUM-10557 (NUM_10.3_wM_RealmServer_Fix_1)

A node could get stuck in recovery during cluster formation phase. 

NUM-10555 (NUM_10.3_wM_RealmServer_Fix_1)

A cluster formation issue could arise due to a synchronization issue.
This issue is now resolved.

NUM-10554 (NUM_10.3_wM_RealmServer_Fix_1)

Sporadic deadlocks and lockouts could occur during cluster creation phase. Those known issues are 
now resolved. 

NUM-10456 (NUM_10.3_wM_RealmServer_Fix_1)

Change source code defaults to wm profile.
This issue has been resolved.


8.0 Installation

If you are installing a Universal Messaging server fix, please shut down any running Universal
Messaging server instances before installing.
In case the Universal Messaging server instance is a member of a cluster, please apply the
procedure for installing rolling cluster updates described below(*).

If you are installing a Universal Messaging client fix, please shut down any applications such as
Integration server or Enterprise Manager which are using client libraries.

If you are installing a Universal Messaging Enterprise Manager fix, you do not need to shut down the
Universal Messaging server.

Install using the Software AG Update Manager or Command Central (9.5 or later). For instructions,
see the corresponding documentation at http://documentation.softwareag.com.

(*)Since 10.3 Fix 14 Universal Messaging cluster is supporting rolling updates for its nodes.
There is a strong recommendation to start applying the new fixes on the Slave nodes and leave the
Master node to be the last one.

For each single node that is part of the cluster the customer needs to follow the steps:
1) Shutdown the node. The node will be disconnected from the cluster.
2) Apply the specific fix
3) Start the node. It will be connected back again to the cluster and will be operational
4) Apply the same procedure from 1-3 steps on the next node

In order to have truly no downtime, the client that talks to the Universal Messaging cluster should
also be implemented in a way that it tolerates brief connection loss. A rolling update in a cluster
still means that:
a) When the node that a client is connected to is brought down for maintenance, the connection to
this server, and respectively the cluster, will be lost and has to be re-established. The client
should be able to handle this seamlessly.
b) When the master node of the cluster is brought down for maintenance, the cluster will briefly
go offline until a new master is elected. This is usually a very short time window, but still,
similarly to point a). The client applications should be able to handle it in order to have truly
no downtime from the point of view of the end user.



9.0 Uninstallation

If you are uninstalling a Universal Messaging server fix, please shut down any running Universal 
Messaging server instances before uninstalling. Also, if the Universal Messaging server instance is 
a member of a cluster, please make sure all server instances in the cluster are shut down prior to 
uninstalling the fixes.

If you are uninstalling a Universal Messaging client fix, please shut down any applications such as 
Integration Server or Enterprise Manager which are using client libraries.

If you are uninstalling a Universal Messaging Enterprise Manager fix, you do not need to shut down 
the Universal Messaging server.

Uninstall using the Software AG Update Manager or Command Central (9.5 or later). For instructions, 
see the corresponding documentation at http://documentation.softwareag.com.

Note: These instructions can only be used to uninstall the most recently installed fix. This action 
will revert your installation to the previously installed fix. You cannot apply these instructions 
to the previously installed fix.



10.0 Globalization

This fix conforms to the internationalization standards of the webMethods product suite and includes 
support for operation in any country, locale, or language as specified in the Installing webMethods 
Products guide. It was not tested with non-English configurations and non-ASCII data. However, this 
fix has no globalization impact and can be applied to systems running in any supported locale or 
configuration.



10.1 Localization

This fix does not require an updated Language Pack. It might contain new messages and these messages
 will appear in English.



11.0 Copyright

Copyright  2022 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, 
and/or its subsidiaries and/or its affiliates and/or their licensors.

The name Software AG and all Software AG product names are either trademarks or registered 
trademarks of Software AG and/or Software AG USA Inc. and/or its subsidiaries and/or its affiliates 
and/or their licensors. Other company and product names mentioned herein may be trademarks of their 
respective owners.

Detailed information on trademarks and patents owned by Software AG and/or its subsidiaries is 
located at http://softwareag.com/licenses .

This software may include portions of third-party products. For third-party copyright notices, 
license terms, additional rights or restrictions, please refer to "License Texts, Copyright Notices 
and Disclaimers of Third Party Products". For certain specific third-party license restrictions, 
please refer to section E of the Legal Notices available under "License Terms and Conditions for Use 
of Software AG Products / Copyright and Trademark Notices of Software AG Products". These documents 
are part of the product documentation, located at http://softwareag.com/licenses and/or in the root 
installation directory of the licensed product(s).

